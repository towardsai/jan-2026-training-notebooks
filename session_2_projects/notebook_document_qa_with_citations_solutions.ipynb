{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Q&A with Citations (RAG-lite)\n",
        "\n",
        "Goal: answer questions using **only provided documents**, with **verifiable citations**.\n",
        "\n",
        "What you’ll practice:\n",
        "- Chunking + retrieval (TF‑IDF)\n",
        "- Prompting for grounded answers\n",
        "- Structured schema for citations + validation\n",
        "- A simple fallback: 'I don’t know'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "**Estimated time:** ~60–90 minutes (with exercises)\n",
        "\n",
        "### Install\n",
        "If needed, install dependencies:\n",
        "```bash\n",
        "pip install -U openai pydantic pandas numpy scikit-learn\n",
        "```\n",
        "\n",
        "### Environment\n",
        "Set your API key:\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"...\"\n",
        "```\n",
        "\n",
        "> **Note:** All example data in this notebook is synthetic (safe to share in training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "assert os.getenv('OPENAI_API_KEY'), \"Set OPENAI_API_KEY in your environment\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports + API client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()  # uses OPENAI_API_KEY from env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Example documents (synthetic policy excerpts)\n",
        "\n",
        "Replace these with real NYPL docs later (handbook, policies, runbooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "docs = {\n",
        "  \"Remote Work Policy\": \"\"\"Remote work is permitted up to two days per week for eligible roles.\n",
        "Employees must be reachable during core hours (10am–4pm) and follow security guidance for devices.\n",
        "Managers may revoke remote privileges if performance or security issues arise.\"\"\",\n",
        "  \"Incident Response Playbook\": \"\"\"If a public-facing service returns repeated 5xx errors, declare an incident and page on-call.\n",
        "Within 15 minutes, provide an initial status update including scope, severity, and next steps.\n",
        "After mitigation, conduct a post-incident review within 5 business days.\"\"\",\n",
        "  \"Patron Privacy\": \"\"\"Patron personally identifiable information (PII) must not be shared in public channels.\n",
        "When discussing tickets, redact names, emails, phone numbers, and library card numbers.\"\"\"\n",
        "}\n",
        "\n",
        "pd.DataFrame([{\"doc\":k, \"text\":v[:120]+\"...\"} for k,v in docs.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chunking + retrieval (TF‑IDF)\n",
        "\n",
        "We split documents into short chunks and retrieve the top-k most relevant.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def chunk_text(text: str, max_chars: int = 280):\n",
        "    chunks=[]\n",
        "    cur=\"\"\n",
        "    for sent in text.splitlines():\n",
        "        sent=sent.strip()\n",
        "        if not sent: \n",
        "            continue\n",
        "        if len(cur)+len(sent)+1 <= max_chars:\n",
        "            cur = (cur+\" \"+sent).strip()\n",
        "        else:\n",
        "            chunks.append(cur)\n",
        "            cur = sent\n",
        "    if cur:\n",
        "        chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "chunks=[]\n",
        "for doc_name, text in docs.items():\n",
        "    for i, ch in enumerate(chunk_text(text)):\n",
        "        chunks.append({\"doc\": doc_name, \"chunk_id\": f\"{doc_name}::c{i}\", \"text\": ch})\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "df_chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(df_chunks[\"text\"].tolist())\n",
        "\n",
        "def retrieve(query: str, k: int = 3):\n",
        "    q = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q, X).ravel()\n",
        "    top = sims.argsort()[::-1][:k]\n",
        "    return df_chunks.iloc[top].assign(score=sims[top])\n",
        "\n",
        "retrieve(\"What is the incident update timing?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define an answer schema with citations\n",
        "\n",
        "We force the model to cite chunk IDs it used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Citation(BaseModel):\n",
        "    chunk_id: str = Field(..., description=\"Must be one of the provided chunk_ids\")\n",
        "    quote: str = Field(..., description=\"Short verbatim quote (<=25 words) from the chunk\")\n",
        "    why_relevant: str = Field(..., description=\"Why this quote supports the answer\")\n",
        "\n",
        "class GroundedAnswer(BaseModel):\n",
        "    answer: str\n",
        "    citations: List[Citation]\n",
        "    uncertainty: Optional[str] = Field(None, description=\"If unsure, say what is missing\")\n",
        "\n",
        "def validate_citations(citations, allowed_ids):\n",
        "    bad = [c.chunk_id for c in citations if c.chunk_id not in allowed_ids]\n",
        "    if bad:\n",
        "        raise ValueError(f\"Bad chunk_ids in citations: {bad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ask a question (grounded)\n",
        "\n",
        "We send only the retrieved chunks as context.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SYSTEM = \"\"\"You answer questions using ONLY the provided CONTEXT.\n",
        "If the answer is not in context, say you don't know and explain what you'd need.\n",
        "Always include citations to the exact chunk_ids you used.\"\"\"\n",
        "\n",
        "def answer_question(query: str, k: int = 3) -> GroundedAnswer:\n",
        "    retrieved = retrieve(query, k=k)\n",
        "    allowed_ids = set(retrieved[\"chunk_id\"].tolist())\n",
        "\n",
        "    context = \"\\n\\n\".join([f\"[{row.chunk_id}] {row.text}\" for row in retrieved.itertuples()])\n",
        "    user = f\"\"\"QUESTION: {query}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "    resp = client.responses.parse(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        input=[{\"role\":\"system\",\"content\":SYSTEM},{\"role\":\"user\",\"content\":user}],\n",
        "        text_format=GroundedAnswer\n",
        "    )\n",
        "    out = resp.output_parsed\n",
        "    validate_citations(out.citations, allowed_ids)\n",
        "    return out\n",
        "\n",
        "ans = answer_question(\"When do we need to do a post-incident review?\", k=3)\n",
        "ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Pretty-print with citations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(ans.answer)\n",
        "print()\n",
        "for c in ans.citations:\n",
        "    print(\"-\", c.chunk_id, \":\", c.quote)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCISE — SOLUTION\n",
        "# Add a hard guardrail that refuses answers if `citations` is empty.\n",
        "\n",
        "def enforce_non_empty_citations(ans: GroundedAnswer) -> GroundedAnswer:\n",
        "    if not ans.citations:\n",
        "        raise ValueError(\"No citations provided; refuse to answer.\")\n",
        "    return ans\n",
        "\n",
        "ans2 = answer_question(\"What are core hours?\", k=3)\n",
        "enforce_non_empty_citations(ans2)\n",
        "ans2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# EXERCISE — SOLUTION\n",
        "# Improve chunking: split by sentences (.) and aim for ~2–3 sentences per chunk. Rebuild the index and compare retrieval quality on 2 queries.\n",
        "\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def chunk_text_sentences(text: str, max_sents: int = 3):\n",
        "    sents=[s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
        "    chunks=[]\n",
        "    for i in range(0, len(sents), max_sents):\n",
        "        chunks.append(\" \".join(sents[i:i+max_sents]))\n",
        "    return chunks\n",
        "\n",
        "chunks=[]\n",
        "for doc_name, text in docs.items():\n",
        "    for i, ch in enumerate(chunk_text_sentences(text, max_sents=3)):\n",
        "        chunks.append({\"doc\": doc_name, \"chunk_id\": f\"{doc_name}::s{i}\", \"text\": ch})\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(df_chunks[\"text\"].tolist())\n",
        "\n",
        "def retrieve2(query: str, k: int = 3):\n",
        "    q = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q, X).ravel()\n",
        "    top = sims.argsort()[::-1][:k]\n",
        "    return df_chunks.iloc[top].assign(score=sims[top])\n",
        "\n",
        "print(\"Query 1:\")\n",
        "display(retrieve2(\"When do we need a post-incident review?\", k=3))\n",
        "print(\"Query 2:\")\n",
        "display(retrieve2(\"What are core hours for remote work?\", k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCISE — SOLUTION\n",
        "# Add a fallback: if top similarity score < 0.15, return 'I don't know' without calling the model.\n",
        "\n",
        "def answer_question_with_threshold(query: str, k: int = 3, thresh: float = 0.15):\n",
        "    retrieved = retrieve(query, k=k)\n",
        "    if retrieved[\"score\"].max() < thresh:\n",
        "        return GroundedAnswer(answer=\"I don't know based on the provided documents.\", citations=[], uncertainty=\"Top retrieval score too low; need more relevant documents.\")\n",
        "    out = answer_question(query, k=k)\n",
        "    return out\n",
        "\n",
        "print(answer_question_with_threshold(\"What is the cafeteria menu?\", k=3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "title": "Document QA (solutions)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}