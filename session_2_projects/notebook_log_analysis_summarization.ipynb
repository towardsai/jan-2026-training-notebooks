{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c5f0e2",
   "metadata": {},
   "source": [
    "# Log Analysis & Anomaly Summarization\n",
    "\n",
    "**Goal:** Turn a day of web/app logs into a structured incident summary (counts, anomalies, and next steps).\n",
    "\n",
    "**No GPU required.**\n",
    "\n",
    "You’ll learn:\n",
    "- Simple log parsing + redaction\n",
    "- Long-context summarization patterns\n",
    "- Structured outputs for machine-readable incident reports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d1b5a",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "You’ll need `openai`, `pydantic`, and `pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade openai pydantic pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182e2b2",
   "metadata": {},
   "source": [
    "## 2. Imports + API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, confloat\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"Missing OPENAI_API_KEY. Set it and re-run.\")\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7365d",
   "metadata": {},
   "source": [
    "## 3. Synthetic logs (Nginx + app errors)\n",
    "\n",
    "We’ll use small synthetic logs so this notebook runs anywhere.\n",
    "In real usage, you’d load from files / S3 / logging tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "def synth_access_log(n=200):\n",
    "    endpoints = [\"/search\", \"/locations\", \"/events\", \"/api/catalog\", \"/account/login\"]\n",
    "    statuses = [200, 200, 200, 200, 404, 500, 502]\n",
    "    agents = [\"Mozilla/5.0\", \"curl/8.0\", \"NYPLMobile/2.1\"]\n",
    "    rows=[]\n",
    "    for i in range(n):\n",
    "        ep = random.choice(endpoints)\n",
    "        status = random.choice(statuses)\n",
    "        ms = int(abs(random.gauss(120, 60)))\n",
    "        if ep == \"/search\" and random.random() < 0.25:\n",
    "            status = random.choice([500,502])\n",
    "            ms = int(abs(random.gauss(900, 250)))\n",
    "        ip = f\"192.0.2.{random.randint(1,254)}\"  # RFC 5737 TEST-NET-1\n",
    "        ua = random.choice(agents)\n",
    "        rows.append(f'{ip} - - [09/Jan/2026:14:{random.randint(0,59):02d}:{random.randint(0,59):02d} -0500] \"GET {ep} HTTP/1.1\" {status} 1234 \"-\" \"{ua}\" {ms}')\n",
    "    return rows\n",
    "\n",
    "def synth_app_log(n=60):\n",
    "    msgs = [\n",
    "        \"DB timeout in queryCatalog() after 5s\",\n",
    "        \"Redis connection refused\",\n",
    "        \"Upstream read timeout\",\n",
    "        \"JWT validation failed: token expired\",\n",
    "        \"Rate limit exceeded for /api/catalog\",\n",
    "        \"Template render error in due-date notice\",\n",
    "    ]\n",
    "    rows=[]\n",
    "    for i in range(n):\n",
    "        level = random.choices([\"INFO\",\"WARN\",\"ERROR\"], weights=[0.55,0.25,0.20])[0]\n",
    "        msg = random.choice(msgs)\n",
    "        if \"timeout\" in msg and random.random() < 0.5:\n",
    "            level = \"ERROR\"\n",
    "        rows.append(f\"2026-01-09T14:{random.randint(0,59):02d}:{random.randint(0,59):02d}-05:00 {level} {msg}\")\n",
    "    return rows\n",
    "\n",
    "access_logs = synth_access_log()\n",
    "app_logs = synth_app_log()\n",
    "\n",
    "print(access_logs[0])\n",
    "print(app_logs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7169f0e",
   "metadata": {},
   "source": [
    "## 4. Redaction helpers\n",
    "\n",
    "Always remove secrets/tokens before sending logs to a model.\n",
    "Here we demonstrate simple regex-based redaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336298a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_PATTERNS = [\n",
    "    (re.compile(r\"(?i)authorization:\\s*bearer\\s+[A-Za-z0-9._-]+\"), \"authorization: Bearer [REDACTED]\"),\n",
    "    (re.compile(r\"(?i)api[_-]?key\\s*[:=]\\s*[A-Za-z0-9._-]+\"), \"api_key=[REDACTED]\"),\n",
    "    (re.compile(r\"(?i)jwt\\s+[A-Za-z0-9._-]+\\.[A-Za-z0-9._-]+\\.[A-Za-z0-9._-]+\"), \"JWT [REDACTED]\"),\n",
    "]\n",
    "\n",
    "def redact(text: str) -> str:\n",
    "    out = text\n",
    "    for pat, repl in SECRET_PATTERNS:\n",
    "        out = pat.sub(repl, out)\n",
    "    return out\n",
    "\n",
    "# Example\n",
    "redact(\"authorization: Bearer abc.def.ghi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17b285",
   "metadata": {},
   "source": [
    "## 5. Quick parsing (counts by status + endpoint)\n",
    "\n",
    "This is a cheap, deterministic baseline. We’ll feed these stats + samples to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_RE = re.compile(r'\"GET (?P<path>\\S+) HTTP/1\\.1\" (?P<status>\\d{3}) .* (?P<ms>\\d+)$')\n",
    "\n",
    "def parse_access(log_lines: List[str]) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for line in log_lines:\n",
    "        m = ACCESS_RE.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"path\": m.group(\"path\"),\n",
    "            \"status\": int(m.group(\"status\")),\n",
    "            \"latency_ms\": int(m.group(\"ms\")),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_access = parse_access(access_logs)\n",
    "df_access.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4482bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_counts = df_access[\"status\"].value_counts().to_dict()\n",
    "endpoint_counts = df_access[\"path\"].value_counts().to_dict()\n",
    "\n",
    "slow = df_access.sort_values(\"latency_ms\", ascending=False).head(10)\n",
    "\n",
    "status_counts, list(endpoint_counts.items())[:3], slow.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d08eab",
   "metadata": {},
   "source": [
    "## 6. Define an incident summary schema\n",
    "\n",
    "We want machine-readable output that could power a dashboard, Jira ticket, or Slack post.\n",
    "Structured Outputs ensure schema adherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndpointHotspot(BaseModel):\n",
    "    path: str\n",
    "    count: int\n",
    "    notes: str\n",
    "\n",
    "class Anomaly(BaseModel):\n",
    "    title: str\n",
    "    evidence: List[str] = Field(..., description=\"Short evidence strings based on provided logs/stats.\")\n",
    "    likely_cause: str\n",
    "    suggested_actions: List[str]\n",
    "\n",
    "class LogIncidentSummary(BaseModel):\n",
    "    overview: str\n",
    "    key_metrics: Dict[str, int] = Field(..., description=\"Core counts like total_requests, errors_5xx, errors_4xx.\")\n",
    "    top_endpoints: List[EndpointHotspot]\n",
    "    anomalies: List[Anomaly]\n",
    "    confidence: confloat(ge=0, le=1)\n",
    "    followup_questions: List[str] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a1d89",
   "metadata": {},
   "source": [
    "## 7. Long-context prompt (stats + samples)\n",
    "\n",
    "We’ll send:\n",
    "- Aggregated stats (fast + cheap)\n",
    "- A small sample of raw log lines (for flavor/evidence)\n",
    "\n",
    "Tip: For very large logs, summarize per chunk and then summarize summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"You are an SRE-style assistant for a library web platform.\n",
    "Write an incident-style summary based ONLY on the provided stats and log samples.\n",
    "\n",
    "Rules:\n",
    "- Do not invent metrics that aren't provided.\n",
    "- If something is unclear, ask follow-up questions.\n",
    "- Use the schema exactly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_payload(access_logs: List[str], app_logs: List[str], max_samples: int = 40) -> str:\n",
    "    # redact + sample\n",
    "    a = [redact(x) for x in access_logs[:max_samples]]\n",
    "    b = [redact(x) for x in app_logs[:max_samples]]\n",
    "    return (\n",
    "        \"AGGREGATED METRICS\\n\"\n",
    "        f\"status_counts={status_counts}\\n\"\n",
    "        f\"endpoint_counts={dict(list(endpoint_counts.items())[:10])}\\n\"\n",
    "        f\"top_slowest={slow.to_dict(orient='records')}\\n\\n\"\n",
    "        \"ACCESS LOG SAMPLES\\n\" + \"\\n\".join(a) + \"\\n\\n\"\n",
    "        \"APP LOG SAMPLES\\n\" + \"\\n\".join(b)\n",
    "    )\n",
    "\n",
    "payload = build_payload(access_logs, app_logs)\n",
    "payload[:600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_logs(payload: str, model: str = \"gpt-4o-mini\") -> LogIncidentSummary:\n",
    "    response = client.responses.parse(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": payload},\n",
    "        ],\n",
    "        text_format=LogIncidentSummary,\n",
    "    )\n",
    "    return response.output_parsed\n",
    "\n",
    "summary = summarize_logs(payload)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e2ccc",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "### EXERCISE 1: Add a redaction for email addresses\n",
    "Write a regex that redacts `name@example.com`.\n",
    "\n",
    "### EXERCISE 2: Add chunking for long logs\n",
    "Implement a function that:\n",
    "- Splits logs into chunks of N lines\n",
    "- Summarizes each chunk\n",
    "- Produces a final summary over chunk summaries\n",
    "\n",
    "### EXERCISE 3: Add a 'severity' field\n",
    "Extend the schema with `severity` (low/medium/high) based on 5xx rate and latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd501fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE STARTER CELL\n",
    "\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
