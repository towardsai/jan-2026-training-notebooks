{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Log Analysis & Anomaly Summarization\n",
        "\n",
        "Goal: summarize noisy logs into an **incident-ready report** with counts, suspected causes, and next actions.\n",
        "\n",
        "What you’ll practice:\n",
        "- Parsing logs with Python (baseline)\n",
        "- Long-context prompting pattern (stats + samples)\n",
        "- Structured incident report outputs\n",
        "- Batch/day partitioning pattern\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "**Estimated time:** ~60–90 minutes (with exercises)\n",
        "\n",
        "### Install\n",
        "If needed, install dependencies:\n",
        "```bash\n",
        "pip install -U openai pydantic pandas numpy scikit-learn\n",
        "```\n",
        "\n",
        "### Environment\n",
        "Set your API key:\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"...\"\n",
        "```\n",
        "\n",
        "> **Note:** All example data in this notebook is synthetic (safe to share in training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "assert os.getenv('OPENAI_API_KEY'), \"Set OPENAI_API_KEY in your environment\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports + API client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()  # uses OPENAI_API_KEY from env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal, Optional, Dict\n",
        "import random\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter, defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate synthetic logs\n",
        "\n",
        "We’ll simulate a day of Nginx access logs + app error logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(7)\n",
        "\n",
        "def make_access_line(ts, status, path, rt_ms, ip=\"10.2.3.4\", ua=\"Mozilla/5.0\"):\n",
        "    return f'{ip} - - [{ts}] \"GET {path} HTTP/1.1\" {status} 1234 \"-\" \"{ua}\" {rt_ms}ms'\n",
        "\n",
        "def make_error_line(ts, level, msg, svc=\"api\"):\n",
        "    return f'[{ts}] {svc} {level}: {msg}'\n",
        "\n",
        "paths = [\"/\", \"/search\", \"/digital-collections/search\", \"/login\", \"/account/reset\"]\n",
        "statuses = [200,200,200,404,500,502]\n",
        "error_msgs = [\n",
        "    \"upstream timeout while reading response header\",\n",
        "    \"database connection pool exhausted\",\n",
        "    \"cache miss storm detected\",\n",
        "    \"token validation failed\",\n",
        "    \"rate limit from upstream dependency\"\n",
        "]\n",
        "\n",
        "def gen_logs(n=800):\n",
        "    logs=[]\n",
        "    for i in range(n):\n",
        "        ts=f\"09/Jan/2026:12:{i%60:02d}:{(i*7)%60:02d} -0500\"\n",
        "        path=random.choice(paths)\n",
        "        status=random.choices(statuses, weights=[70,70,70,10,5,5])[0]\n",
        "        rt_ms=int(max(5, random.gauss(120, 80)))\n",
        "        logs.append(make_access_line(ts, status, path, rt_ms))\n",
        "        # sprinkle errors\n",
        "        if status in (500,502) and random.random() < 0.7:\n",
        "            logs.append(make_error_line(ts, \"ERROR\", random.choice(error_msgs)))\n",
        "    return logs\n",
        "\n",
        "logs = gen_logs()\n",
        "print(\"Total lines:\", len(logs))\n",
        "print(\"\\n\".join(logs[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Parse logs (baseline)\n",
        "\n",
        "We’ll extract:\n",
        "- counts by status\n",
        "- top endpoints for 5xx\n",
        "- rough latency percentiles\n",
        "- error message counts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ACCESS_RE = re.compile(r'\\] \"GET (?P<path>[^ ]+) HTTP/1\\.1\" (?P<status>\\d{3}).* (?P<rt>\\d+)ms$')\n",
        "ERROR_RE  = re.compile(r'\\] (?P<svc>\\w+) (?P<level>\\w+): (?P<msg>.*)$')\n",
        "\n",
        "access=[]\n",
        "errors=[]\n",
        "for line in logs:\n",
        "    m=ACCESS_RE.search(line)\n",
        "    if m:\n",
        "        access.append({\"path\": m.group(\"path\"), \"status\": int(m.group(\"status\")), \"rt_ms\": int(m.group(\"rt\"))})\n",
        "        continue\n",
        "    m2=ERROR_RE.search(line)\n",
        "    if m2:\n",
        "        errors.append({\"svc\": m2.group(\"svc\"), \"level\": m2.group(\"level\"), \"msg\": m2.group(\"msg\")})\n",
        "\n",
        "df_access = pd.DataFrame(access)\n",
        "df_errors = pd.DataFrame(errors)\n",
        "\n",
        "df_access.head(), df_errors.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "status_counts = df_access[\"status\"].value_counts().sort_index()\n",
        "top_5xx_paths = df_access[df_access[\"status\"].isin([500,502])][\"path\"].value_counts().head(10)\n",
        "lat_pcts = df_access[\"rt_ms\"].quantile([0.5,0.9,0.95,0.99]).to_dict()\n",
        "err_counts = df_errors[\"msg\"].value_counts().head(10)\n",
        "\n",
        "status_counts, top_5xx_paths, lat_pcts, err_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define a structured incident report schema\n",
        "\n",
        "This is what you’d hand to an on-call engineer or attach to a ticket.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class IncidentReport(BaseModel):\n",
        "    summary: str\n",
        "    severity: Literal[\"SEV1\",\"SEV2\",\"SEV3\",\"INFO\"]\n",
        "    suspected_root_causes: List[str]\n",
        "    key_metrics: Dict[str, str]\n",
        "    top_errors: List[str]\n",
        "    top_impacted_endpoints: List[str]\n",
        "    recommended_actions: List[str]\n",
        "    confidence: float = Field(..., ge=0, le=1)\n",
        "    needs_human_review: bool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Long-context prompt (stats + samples)\n",
        "\n",
        "We give the model:\n",
        "1) aggregate stats (cheap, informative)\n",
        "2) a small sample of raw lines (for flavor)\n",
        "3) clear constraints + schema\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SYSTEM = \"\"\"You are an SRE assistant. Produce an incident report from log stats + samples.\n",
        "Be conservative: if evidence is weak, set needs_human_review=true and confidence<=0.6.\n",
        "Avoid hallucinating: only infer causes supported by the provided errors/metrics.\"\"\"\n",
        "\n",
        "def build_input(status_counts, top_5xx_paths, lat_pcts, err_counts, sample_lines):\n",
        "    return f\"\"\"Log stats:\n",
        "Status counts: {status_counts.to_dict()}\n",
        "Top 5xx endpoints: {top_5xx_paths.to_dict()}\n",
        "Latency percentiles (ms): {lat_pcts}\n",
        "Top error messages: {err_counts.to_dict()}\n",
        "\n",
        "Sample raw lines (first 40):\n",
        "{chr(10).join(sample_lines)}\n",
        "\"\"\"\n",
        "\n",
        "report = client.responses.parse(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    input=[\n",
        "        {\"role\":\"system\",\"content\": SYSTEM},\n",
        "        {\"role\":\"user\",\"content\": build_input(status_counts, top_5xx_paths, lat_pcts, err_counts, logs[:40])}\n",
        "    ],\n",
        "    text_format=IncidentReport\n",
        ").output_parsed\n",
        "\n",
        "report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Turn the report into a ticket-ready payload\n",
        "\n",
        "Often you’ll want a concise block for a Jira/ServiceNow update.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ticket_update = f\"\"\"INCIDENT SUMMARY\n",
        "Severity: {report.severity}\n",
        "Summary: {report.summary}\n",
        "\n",
        "Key metrics:\n",
        "- \"\"\" + \"\\n- \".join([f\"{k}: {v}\" for k,v in report.key_metrics.items()]) + f\"\"\"\n",
        "\n",
        "Top errors:\n",
        "- \"\"\" + \"\\n- \".join(report.top_errors[:5]) + f\"\"\"\n",
        "\n",
        "Recommended actions:\n",
        "- \"\"\" + \"\\n- \".join(report.recommended_actions[:5]) + \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "print(ticket_update)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exercises\n",
        "\n",
        "These extend the notebook into production-ish patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCISE\n",
        "# Add a redaction step that replaces email addresses with '<EMAIL>' and long numeric IDs (8+ digits) with '<ID>' before sampling raw lines.\n",
        "\n",
        "def redact(line: str) -> str:\n",
        "    # TODO: implement redaction\n",
        "    raise NotImplementedError(\"TODO\")\n",
        "\n",
        "# Apply redact() to logs and rebuild the sample_lines\n",
        "raise NotImplementedError(\"TODO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCISE\n",
        "# Compute an 'anomaly score' for each endpoint: (5xx_rate * median_latency). Print top 5 endpoints by score.\n",
        "\n",
        "# TODO: compute per-endpoint 5xx rate and median latency, then score\n",
        "raise NotImplementedError(\"TODO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCISE\n",
        "# Modify the model prompt to include the anomaly scores and ask it to reference the highest-scoring endpoint in the report.\n",
        "\n",
        "# TODO: add anomaly score summary into build_input and re-run report generation\n",
        "raise NotImplementedError(\"TODO\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "title": "Log Analysis (exercise)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}