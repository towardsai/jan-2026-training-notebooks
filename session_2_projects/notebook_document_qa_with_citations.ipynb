{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f55a857",
   "metadata": {},
   "source": [
    "# Document Q&A with Citations (RAG-lite)\n",
    "\n",
    "**Goal:** Answer questions with **verifiable citations** to the exact document passages provided.\n",
    "\n",
    "**No GPU required.**\n",
    "\n",
    "You’ll learn:\n",
    "- Basic document chunking\n",
    "- Lightweight retrieval (TF‑IDF)\n",
    "- Grounded answers with a citation schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb2a4d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd14572",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade openai pydantic pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bce042",
   "metadata": {},
   "source": [
    "## 2. Imports + API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d01cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, confloat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"Missing OPENAI_API_KEY. Set it and re-run.\")\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a9d36",
   "metadata": {},
   "source": [
    "## 3. Example documents (synthetic)\n",
    "\n",
    "In your real workflow, these could be:\n",
    "- internal runbooks\n",
    "- policy docs\n",
    "- incident playbooks\n",
    "\n",
    "Here we keep them synthetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35aed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        \"source_id\": \"policy-privacy-v1\",\n",
    "        \"title\": \"Patron Privacy Policy (Excerpt)\",\n",
    "        \"text\": \"\"\"1. Purpose\n",
    "NYPL protects patron privacy and limits disclosure of personally identifying information.\n",
    "\n",
    "2. Data handling\n",
    "Staff should only access patron data necessary for service delivery. Do not export patron records to personal devices.\n",
    "\n",
    "3. Disclosure\n",
    "Do not share patron activity data except as required by law or authorized policy.\n",
    "\n",
    "4. Retention\n",
    "Operational logs containing patron identifiers should be retained only as long as necessary and then deleted per retention schedules.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"source_id\": \"runbook-search-v1\",\n",
    "        \"title\": \"Search Service Runbook (Excerpt)\",\n",
    "        \"text\": \"\"\"1. Symptoms\n",
    "Common symptoms include 5xx errors on /search and elevated latency.\n",
    "\n",
    "2. First checks\n",
    "Check deploy status, error budget alerts, and upstream dependency health (database, cache).\n",
    "\n",
    "3. Mitigation\n",
    "If error rate is high, rollback the last deploy and scale search workers. If DB timeouts appear, reduce query load and investigate indexes.\n",
    "\n",
    "4. Communication\n",
    "Post an incident update including impact, mitigations, and next update time.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"source_id\": \"policy-access-v1\",\n",
    "        \"title\": \"Staff Account Access Policy (Excerpt)\",\n",
    "        \"text\": \"\"\"1. Account lockouts\n",
    "If staff SSO accounts are locked, verify identity via approved channels and reset through the standard access workflow.\n",
    "\n",
    "2. Least privilege\n",
    "Grant only the minimum roles needed to perform job functions.\n",
    "\n",
    "3. Audit\n",
    "All access changes must be logged with a ticket reference and approver.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "pd.DataFrame([{k:v for k,v in d.items() if k!='text'} | {\"chars\": len(d[\"text\"])} for d in docs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e5da1",
   "metadata": {},
   "source": [
    "## 4. Chunking + retrieval (TF‑IDF)\n",
    "\n",
    "This is a simple “RAG-lite” approach:\n",
    "- split docs into chunks\n",
    "- retrieve top-k chunks\n",
    "- ask the model to answer using only those chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 280, overlap: int = 40) -> List[str]:\n",
    "    # very simple character-based chunking (fine for demos)\n",
    "    chunks=[]\n",
    "    start=0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if end == len(text):\n",
    "            break\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "chunks=[]\n",
    "for d in docs:\n",
    "    for i, c in enumerate(chunk_text(d[\"text\"])):\n",
    "        chunks.append({\n",
    "            \"source_id\": d[\"source_id\"],\n",
    "            \"title\": d[\"title\"],\n",
    "            \"chunk_id\": f'{d[\"source_id\"]}::chunk{i}',\n",
    "            \"text\": c\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(chunks)\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_chunks[\"text\"].tolist())\n",
    "\n",
    "def retrieve(query: str, k: int = 4) -> pd.DataFrame:\n",
    "    q = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(q, X).flatten()\n",
    "    top_idx = sims.argsort()[::-1][:k]\n",
    "    out = df_chunks.iloc[top_idx].copy()\n",
    "    out[\"score\"] = sims[top_idx]\n",
    "    return out.sort_values(\"score\", ascending=False)\n",
    "\n",
    "retrieve(\"What should we do if /search is returning 5xx and latency is high?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a635cb",
   "metadata": {},
   "source": [
    "## 5. Define an answer schema with citations\n",
    "\n",
    "We will force the model to produce:\n",
    "- an answer\n",
    "- citations that point to specific chunks we provided\n",
    "\n",
    "Structured Outputs helps keep the response reliably machine-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Citation(BaseModel):\n",
    "    chunk_id: str = Field(..., description=\"Must be one of the provided chunk_id values.\")\n",
    "    source_id: str\n",
    "    title: str\n",
    "    quoted_evidence: str = Field(..., description=\"A short excerpt (<=25 words) from the chunk that supports the answer.\")\n",
    "\n",
    "class GroundedAnswer(BaseModel):\n",
    "    answer: str\n",
    "    citations: List[Citation]\n",
    "    could_not_answer: bool = Field(..., description=\"True if the retrieved chunks do not contain enough info.\")\n",
    "    confidence: confloat(ge=0, le=1)\n",
    "    followups: List[str] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafdcac",
   "metadata": {},
   "source": [
    "## 6. Ask a question (grounded)\n",
    "\n",
    "We pass only the retrieved chunks to the model, and instruct it to cite them.\n",
    "\n",
    "Note: Always validate that returned `chunk_id`s are from the provided set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"You answer questions for NYPL developers.\n",
    "You MUST use only the provided document chunks as your source of truth.\n",
    "\n",
    "Rules:\n",
    "- If the chunks do not contain the answer, set could_not_answer=true and ask follow-up questions.\n",
    "- Every non-trivial claim must be supported by at least one citation.\n",
    "- quoted_evidence must be a short excerpt copied from the chunk.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def answer_with_citations(question: str, k: int = 4, model: str = \"gpt-4o-mini\") -> GroundedAnswer:\n",
    "    ctx = retrieve(question, k=k)\n",
    "    allowed_chunk_ids = set(ctx[\"chunk_id\"].tolist())\n",
    "\n",
    "    context_block = \"\\n\\n\".join(\n",
    "        [f\"[{r.chunk_id}] ({r.source_id} | {r.title})\\n{r.text}\" for r in ctx.itertuples()]\n",
    "    )\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION:\\n{question}\\n\\nCHUNKS:\\n{context_block}\"},\n",
    "        ],\n",
    "        text_format=GroundedAnswer,\n",
    "    )\n",
    "    out = response.output_parsed\n",
    "\n",
    "    # Validate citations\n",
    "    bad = [c.chunk_id for c in out.citations if c.chunk_id not in allowed_chunk_ids]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Model returned citations to unknown chunks: {bad}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "answer_with_citations(\"If /search is throwing 5xx errors, what are the first checks and mitigations?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff6756",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "### EXERCISE 1: Add a 'strict mode' fallback\n",
    "If validation fails (unknown chunk_id), re-run once with a stronger system instruction.\n",
    "\n",
    "### EXERCISE 2: Add a second retrieval method\n",
    "Try a keyword overlap scorer and compare top-k chunks to TF‑IDF.\n",
    "\n",
    "### EXERCISE 3: Add a new document\n",
    "Add a synthetic 'Retention Schedule' doc, re-index, and ask a retention question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff661fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE STARTER CELL\n",
    "\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
