{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr9BTSBZsn-T"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwd9zOyg5YCJ"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JitqPTOI7Qj-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"microsoft/Phi-4-mini-instruct\",\n",
        "    local_dir=\"./models/Phi-4-mini-instruct\",\n",
        "    local_dir_use_symlinks=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XbEKxlv5YAE"
      },
      "outputs": [],
      "source": [
        "!python convert_hf_to_gguf.py ./models/Phi-4-mini-instruct/ \\\n",
        "  --outfile ./models/Phi-4-mini-instruct/ggml-model-f16.gguf \\\n",
        "  --outtype f16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNY3gCA55fxg"
      },
      "outputs": [],
      "source": [
        "!./build/bin/llama-quantize ./models/Phi-4-mini-instruct/ggml-model-f16.gguf \\\n",
        "           ./models/Phi-4-mini-instruct/ggml-model-Q4_K_M.gguf \\\n",
        "           Q4_K_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3FfqnxV8SEE"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# ─── 1) Initialize the model ───────────────────────────────────────────────────\n",
        "model_path = \"/content/llama.cpp/models/Phi-4-mini-instruct/ggml-model-Q4_K_M.gguf\"\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ─── 2) Build a proper chat‐style prompt ────────────────────────────────────────\n",
        "# Phi-4-mini-instruct expects the ChatML format:\n",
        "#\n",
        "#   <|system|>…<|end|><|user|>…<|end|><|assistant|>\n",
        "#\n",
        "# You can also call llm.create_chat_completion to have the library format\n",
        "# it for you under the hood.\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",    \"content\": \"You are a thoughtful, detailed AI assistant.\"},\n",
        "    {\"role\": \"user\",      \"content\": \"What does life mean? Describe in great detail.\"}\n",
        "]\n",
        "\n",
        "# ─── 3) Send it via the chat endpoint ──────────────────────────────────────────\n",
        "response = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    max_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "# ─── 4) Print the assistant’s answer ───────────────────────────────────────────\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
