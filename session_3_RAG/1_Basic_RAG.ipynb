{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tw3tvMs6R-Y"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HaB4G9zr0BYm",
        "outputId": "21649677-4e20-4b50-9c49-89d9329fa9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.5/260.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==2.8.1 google-genai==1.51.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYvUA6CF2Le6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" and \"GOOGLE_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] =  \"<YOUR_GOOGLE_API_KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] =  userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ViVXXIqXBai"
      },
      "outputs": [],
      "source": [
        "# False: Generate the embedding for the dataset. (Associated cost with using OpenAI endpoint)\n",
        "# True: Load the dataset that already has the embedding vectors.\n",
        "load_embedding = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Nzx-cN_bDz"
      },
      "source": [
        "# Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JpI7GiZ--Gw"
      },
      "source": [
        "## Download Dataset (JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT68BDYt-GkG"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6NEJT9S2OoH",
        "outputId": "9d5b0195-379b-415f-cf17-c7e0753a3e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-09 09:45:30--  https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173646 (170K) [text/plain]\n",
            "Saving to: ‘mini-llama-articles.csv’\n",
            "\n",
            "mini-llama-articles 100%[===================>] 169.58K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-12-09 09:45:31 (6.60 MB/s) - ‘mini-llama-articles.csv’ saved [173646/173646]\n",
            "\n",
            "--2025-12-09 09:45:31--  https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles-with_embeddings.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6157740 (5.9M) [text/plain]\n",
            "Saving to: ‘mini-llama-articles-with_embeddings.csv’\n",
            "\n",
            "mini-llama-articles 100%[===================>]   5.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-09 09:45:31 (45.0 MB/s) - ‘mini-llama-articles-with_embeddings.csv’ saved [6157740/6157740]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\n",
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles-with_embeddings.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYDd03Qn_clh"
      },
      "source": [
        "## Read File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bfhs5NMYr4N"
      },
      "outputs": [],
      "source": [
        "# Split the input text into chunks of specified size.\n",
        "def split_into_chunks(text, chunk_size=1024):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i : i + chunk_size])\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcQ7Ge_XCuXa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "chunks = []\n",
        "\n",
        "# Load the file as a CSV\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "            # Skip header row\n",
        "        chunks.extend(split_into_chunks(row[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyJ_C672cSYs",
        "outputId": "0f19ee2a-f3b2-4743-f980-d1de83d3f31e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of articles: 14\n",
            "number of chunks: 174\n"
          ]
        }
      ],
      "source": [
        "print(\"number of articles:\", idx)\n",
        "print(\"number of chunks:\", len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKdFSOb0NXjx",
        "outputId": "d6e85108-75b2-40d0-decd-4bd5d8d46c6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['chunk'], dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the JSON list to a Pandas Dataframe\n",
        "df = pd.DataFrame(chunks, columns=[\"chunk\"])\n",
        "\n",
        "df.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pFDgNdW9rO"
      },
      "source": [
        "# Generate Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfS9w9eQAKyu"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "# Defining a function that converts a text to embedding vector using OpenAI's Ada model.\n",
        "def get_embedding(text):\n",
        "    try:\n",
        "        # Remove newlines\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        res = client.embeddings.create(input=[text], model=\"text-embedding-3-small\")\n",
        "\n",
        "        return res.data[0].embedding\n",
        "\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c2000ee49c8f44b5a1ef068b74089987",
            "3a61fe9936424f5da929423b654ed1f4",
            "fff1f16d8c6a4592aa96f1002a4bebbd",
            "ee0f79b4ad3f4f308fc70809caac2554",
            "1378ea51fba9457e83c4f6ddddf31baa",
            "8492a6550a674a869d94f3b5840980bc",
            "1f6151b481b84633840d6e37f2e468e6",
            "b844483167f540f3b839cd6cad8d1fd0",
            "8b56df223aae4910b1a3ba7f44520119",
            "5af28d5b86e04727a707b7a91b44ce59",
            "dcdb2c294f384685abc98d605cd8bef3"
          ]
        },
        "id": "qC6aeFr3Rmi2",
        "outputId": "087783d4-760a-426d-a024-f8c100b8f7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2000ee49c8f44b5a1ef068b74089987",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Generate embedding\n",
        "if not load_embedding:\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings = []\n",
        "    for index, row in tqdm(df.iterrows()):\n",
        "        # df.at[index, 'embedding'] = get_embedding( row['chunk'] )\n",
        "        embeddings.append(get_embedding(row[\"chunk\"]))\n",
        "\n",
        "    embeddings_values = pd.Series(embeddings)\n",
        "    df.insert(loc=1, column=\"embedding\", value=embeddings_values)\n",
        "\n",
        "# Or, load the embedding from the file.\n",
        "else:\n",
        "    print(\"Loaded the embedding file.\")\n",
        "    # Load the file as a CSV\n",
        "    df = pd.read_csv(\"mini-llama-articles-with_embeddings.csv\")\n",
        "    # Convert embedding column to an array\n",
        "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(eval(x)), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyX9M_n9o2ve"
      },
      "outputs": [],
      "source": [
        "# df.to_csv('mini-llama-articles-with_embeddings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_qrXwImXrXJ"
      },
      "source": [
        "# User Question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGTa7cqCX97q",
        "outputId": "55dbd837-bfc8-44cb-fbeb-4ced519f8220"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the user question, and convert it to embedding.\n",
        "QUESTION = \"How many parameters LLaMA2 model has?\"\n",
        "QUESTION_emb = get_embedding(QUESTION)\n",
        "\n",
        "len(QUESTION_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXNzNWrJYWhU"
      },
      "source": [
        "# Test Cosine Similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxaq-FgLIhIj"
      },
      "source": [
        "Calculating the similarity of embedding representations can help us to find pieces of text that are close to each other. In the following sample you see how the Cosine Similarity metric can identify which sentence could be a possible answer for the given user question. Obviously, the unrelated answer will score lower.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqDWcPd4b-ZI"
      },
      "outputs": [],
      "source": [
        "BAD_SOURCE_emb = get_embedding(\"The sky is blue.\")\n",
        "GOOD_SOURCE_emb = get_embedding(\"LLaMA2 model has a total of 2B parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI00eN86YZKB",
        "outputId": "aa7404ac-9166-4e8a-a729-ece450791278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Bad Response Score: [[0.02578727]]\n",
            "> Good Response Score: [[0.83154609]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# A sample that how a good piece of text can achieve high similarity score compared\n",
        "# to a completely unrelated text.\n",
        "print(\"> Bad Response Score:\", cosine_similarity([QUESTION_emb], [BAD_SOURCE_emb]))\n",
        "print(\"> Good Response Score:\", cosine_similarity([QUESTION_emb], [GOOD_SOURCE_emb]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdJlEtaaJC4I"
      },
      "source": [
        "# Calculate Cosine Similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNPN7OAXemmH",
        "outputId": "64157b40-2e78-42e4-da51-a03878f6610f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.46767499 0.46906372 0.25992494 0.2939657  0.319654   0.40160312\n",
            "  0.41494171 0.45271654 0.45935869 0.1259955  0.11750504 0.01347259\n",
            "  0.22602134 0.21423916 0.10145219 0.33064027 0.1074138  0.34682608\n",
            "  0.16311856 0.08726645 0.3482437  0.22839007 0.19201346 0.26471736\n",
            "  0.24928956 0.34846017 0.24828999 0.32761311 0.41445729 0.41337977\n",
            "  0.46363194 0.38341214 0.46862229 0.35636739 0.35398223 0.3027117\n",
            "  0.29934588 0.29254734 0.40035147 0.4646832  0.39473083 0.41042047\n",
            "  0.44715544 0.43173664 0.35909244 0.33973082 0.51351108 0.20932135\n",
            "  0.40203991 0.32829097 0.42852155 0.48270619 0.45036044 0.34256287\n",
            "  0.32083244 0.42588004 0.2463117  0.18089188 0.23648678 0.34271678\n",
            "  0.3437286  0.20468361 0.1975709  0.22448873 0.21110849 0.42294416\n",
            "  0.26382997 0.30438172 0.33609101 0.38368357 0.23536253 0.24351588\n",
            "  0.37074498 0.28025883 0.49052816 0.53044055 0.37817696 0.43770825\n",
            "  0.37750013 0.39251261 0.30084922 0.41710617 0.4674553  0.45420047\n",
            "  0.35169137 0.21222866 0.42618701 0.31607767 0.44069805 0.52726652\n",
            "  0.50597773 0.49750504 0.44281669 0.35108624 0.39478983 0.4413222\n",
            "  0.20327756 0.27925495 0.15406605 0.19138923 0.1591823  0.2410818\n",
            "  0.22525528 0.19943315 0.26228114 0.35059169 0.36222266 0.15316145\n",
            "  0.27661592 0.45343941 0.33570255 0.2945309  0.381719   0.41733823\n",
            "  0.61951453 0.3868869  0.34361099 0.2827669  0.20139546 0.14609842\n",
            "  0.19517217 0.28226397 0.15626344 0.18060334 0.30277618 0.28139129\n",
            "  0.30262487 0.23778126 0.14555983 0.19749257 0.39240676 0.33001271\n",
            "  0.23546332 0.1570049  0.26884917 0.2648088  0.37826427 0.18126\n",
            "  0.13043953 0.18452304 0.26052189 0.35587602 0.33358688 0.23507334\n",
            "  0.37106625 0.19023676 0.18966691 0.20080808 0.16389302 0.3500477\n",
            "  0.25243468 0.33878725 0.18292758 0.30649191 0.24205178 0.13082281\n",
            "  0.18217836 0.19085239 0.41308701 0.16363392 0.26363609 0.20629575\n",
            "  0.30141703 0.24799972 0.41006581 0.21783605 0.22282313 0.27780672\n",
            "  0.14569464 0.19750236 0.35371121 0.1539897  0.32274227 0.30311173]]\n"
          ]
        }
      ],
      "source": [
        "# The similarity between the questions and each part of the essay.\n",
        "cosine_similarities = cosine_similarity([QUESTION_emb], df[\"embedding\"].tolist())\n",
        "\n",
        "print(cosine_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-XI1_7mhlw4",
        "outputId": "17f239e4-17c6-47c5-b1f7-75265461827f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[114  75  89]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "number_of_chunks_to_retrieve = 3\n",
        "\n",
        "# Sort the scores\n",
        "highest_index = np.argmax(cosine_similarities)\n",
        "\n",
        "# Pick the N highest scored chunks\n",
        "indices = np.argsort(cosine_similarities[0])[::-1][:number_of_chunks_to_retrieve]\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPmhCb9kfB0w",
        "outputId": "93a6f6f0-8727-4807-f0d2-51be25e517d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Chunk 1\n",
            "by Meta that ventures into both the AI and academic spaces. The model aims to help researchers, scientists, and engineers advance their work in exploring AI applications. It will be released under a non-commercial license to prevent misuse, and access will be granted to academic researchers, individuals, and organizations affiliated with the government, civil society, academia, and industry research facilities on a selective case-by-case basis. The sharing of codes and weights allows other researchers to test new approaches in LLMs. The LLaMA models have a range of 7 billion to 65 billion parameters. LLaMA-65B can be compared to DeepMind's Chinchilla and Google's PaLM. Publicly available unlabeled data was used to train these models, and training smaller foundational models require less computing power and resources. LLaMA 65B and 33B have been trained on 1.4 trillion tokens in 20 different languages, and according to the Facebook Artificial Intelligence Research (FAIR) team, the model's performance varies ac\n",
            "----\n",
            "> Chunk 2\n",
            "LLaMA: Meta's new AI tool According to the official release, LLaMA is a foundational language model developed to assist 'researchers and academics' in their work (as opposed to the average web user) to understand and study these NLP models. Leveraging AI in such a way could give researchers an edge in terms of time spent. You may not know this, but this would be Meta's third LLM after Blender Bot 3 and Galactica. However, the two LLMs were shut down soon, and Meta stopped their further development, as it produced erroneous results. Before moving further, it is important to emphasize that LLaMA is NOT a chatbot like ChatGPT. As I mentioned before, it is a 'research tool' for researchers. We can expect the initial versions of LLaMA to be a bit more technical and indirect to use as opposed to the case with ChatGPT, which was very direct, interactive, and a lot easy to use. \"Smaller, more performant models such as LLaMA enable ... research community who don't have access to large amounts of infrastructure to stud\n",
            "----\n",
            "> Chunk 3\n",
            "I. Llama 2: Revolutionizing Commercial Use Unlike its predecessor Llama 1, which was limited to research use, Llama 2 represents a major advancement as an open-source commercial model. Businesses can now integrate Llama 2 into products to create AI-powered applications. Availability on Azure and AWS facilitates fine-tuning and adoption. However, restrictions apply to prevent exploitation. Companies with over 700 million active daily users cannot use Llama 2. Additionally, its output cannot be used to improve other language models.  II. Llama 2 Model Flavors Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters. While 7B, 13B, and 70B have already been released, the 34B model is still awaited. The pretrained variant, trained on a whopping 2 trillion tokens, boasts a context window of 4096 tokens, twice the size of its predecessor Llama 1. Meta also released a Llama 2 fine-tuned model for chat applications that was trained on over 1 million human annota\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "# Look at the highest scored retrieved pieces of text\n",
        "for idx, item in enumerate(df.chunk[indices]):\n",
        "    print(f\"> Chunk {idx+1}\")\n",
        "    print(item)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uvQACqAkHg4"
      },
      "source": [
        "# Augment the Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z89oGs5G6uxY",
        "outputId": "6ab84233-9a08-4a45-957a-fbd8be8fe6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.genai.types import HttpOptions\n",
        "\n",
        "# Initialize client\n",
        "client = genai.Client(api_key=userdata.get('Google_api_key'))\n",
        "\n",
        "# Use the Gemini API to answer the questions based on the retrieved pieces of text.\n",
        "try:\n",
        "    # System instructions for the AI assistant\n",
        "    system_instruction = (\n",
        "        \"You are an assistant and expert in answering questions from a chunks of content. \"\n",
        "        \"Only answer AI-related question, else say that you cannot answer this question.\"\n",
        "    )\n",
        "\n",
        "    # Create a user prompt with the user's question\n",
        "    prompt = (\n",
        "        \"Read the following informations that might contain the context you require to answer the question. You can use the informations starting from the <START_OF_CONTEXT> tag and end with the <END_OF_CONTEXT> tag. Here is the content:\\n\\n<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\n\"\n",
        "        \"Please provide an informative and accurate answer to the following question based on the available context. Be concise and take your time. \\nQuestion: {}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Add the retrieved pieces of text to the prompt\n",
        "    formatted_prompt = prompt.format(\"\".join(df.chunk[indices]), QUESTION)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=formatted_prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            system_instruction=system_instruction,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    res = response.text\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aamim8n6RZHO"
      },
      "source": [
        "## How Augmenting the Prompt can address knowledge cutoff limitations and hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNkcHm8e_xq-"
      },
      "outputs": [],
      "source": [
        "# Consider this as a retrieved chunk\n",
        "# https://ai.meta.com/blog/llama-4-multimodal-intelligence (Summarized Content)\n",
        "\n",
        "Example_chunk =\"\"\"\n",
        "Meta has unveiled the **Llama 4 herd**, a new generation of open-weight, natively multimodal AI models designed to push the boundaries of performance, efficiency, and accessibility. The release includes **Llama 4 Scout** and **Llama 4 Maverick**, alongside a preview of the massive **Llama 4 Behemoth** teacher model. These models introduce advanced **mixture-of-experts (MoE) architectures**, native text–image integration, and record-breaking context lengths, establishing Llama 4 as a major leap in multimodal AI innovation.\n",
        "**Llama 4 Scout** is a **17B active parameter model with 16 experts** (109B total parameters) that offers **10 million token context length**, far exceeding Llama 3’s 128K limit. Scout’s architecture leverages **interleaved attention layers (iRoPE)** and inference-time temperature scaling to achieve superior length generalization, enabling complex tasks like multi-document summarization, codebase reasoning, and long-context retrieval. Despite its smaller size, Scout surpasses prior Llama models and competitors such as Gemma 3 and Gemini 2.0 Flash-Lite in performance, all while being deployable on a **single NVIDIA H100 GPU**. Its multimodal training allows strong image grounding, multi-image reasoning, and visual question answering.\n",
        "**Llama 4 Maverick**, also with **17B active parameters**, uses **128 experts** and totals **400B parameters**, alternating dense and MoE layers for inference efficiency. It rivals or exceeds larger models like **GPT-4o, Gemini 2.0 Flash, and DeepSeek v3** on benchmarks for coding, reasoning, multilinguality, and multimodal tasks. Its efficient design makes it deployable on a single **NVIDIA H100 DGX host**, balancing performance with cost-effectiveness. Maverick was refined through a revamped **post-training pipeline**—lightweight supervised fine-tuning, continuous **online reinforcement learning (RL)** with adaptive difficulty filtering, and direct preference optimization—resulting in superior reasoning, conversational fluency, and multimodal understanding. Maverick’s **chat version achieved an ELO score of 1417 on LMArena**, reflecting best-in-class general assistant capabilities.\n",
        "At the top of the hierarchy, **Llama 4 Behemoth** serves as the **teacher model**, with **288B active parameters, 16 experts, and nearly 2 trillion total parameters**. It outperforms **GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro** on STEM benchmarks like MATH-500 and GPQA Diamond. Behemoth’s scale demanded innovative training strategies, including pruning 95% of supervised fine-tuning data, advanced reinforcement learning with dynamically filtered prompts, and an optimized asynchronous MoE-parallelized RL infrastructure. These techniques boosted reasoning, coding, and multilingual capabilities while maintaining instruction-following reliability.\n",
        "All Llama 4 models are **natively multimodal**, trained with large-scale text, image, and video data, and leverage **Meta’s MetaP technique** for reliable hyperparameter scaling. They support **200 languages**, with 10x more multilingual tokens than Llama 3, and employ **FP8 precision** for efficient training across trillions of tokens. Safety remains a priority: Meta integrates **Llama Guard**, **Prompt Guard**, and **CyberSecEval** for content protection, while **Generative Offensive Agent Testing (GOAT)** automates adversarial red-teaming. Llama 4 also significantly reduces **political and social bias**, refusing fewer prompts and responding more neutrally than Llama 3.\n",
        "In sum, **Llama 4 Scout, Maverick, and Behemoth** represent a major leap in open AI research: compact yet powerful models with unmatched context length, multimodal fluency, reasoning power, and efficiency. By making Scout and Maverick openly available on **llama.com and Hugging Face**, Meta empowers developers, enterprises, and researchers worldwide to build the next generation of AI-driven experiences.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMRy5e5nD3Uo",
        "outputId": "649332be-ec63-47b9-e290-7ce7bed2fb63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Llama 4 models have varying numbers of parameters:\n",
            "\n",
            "*   **Llama 4 Scout:** 17B active parameters with 109B total parameters.\n",
            "*   **Llama 4 Maverick:** 17B active parameters with 400B total parameters.\n",
            "*   **Llama 4 Behemoth:** 288B active parameters with nearly 2 trillion total parameters.\n"
          ]
        }
      ],
      "source": [
        "QUESTION = \"How many parameters does LLaMA 4 model have?\"\n",
        "\n",
        "system_instruction = (\n",
        "    \"You are an assistant and expert in answering questions from a chunks of content. \"\n",
        "    \"Only answer AI-related question, else say that you cannot answer this question.\"\n",
        ")\n",
        "\n",
        "# Combining the context with the user's question\n",
        "prompt = (\n",
        "    \"Read the following informations that might contain the context you require to answer the question. You can use the informations starting from the <START_OF_CONTEXT> tag and end with the <END_OF_CONTEXT> tag. Here is the content:\\n\\n<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\n\"\n",
        "    \"Please provide an informative and accurate answer to the following question based on the available context. Be concise and take your time. \\nQuestion: {}\\nAnswer:\"\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(Example_chunk, QUESTION)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=formatted_prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        system_instruction=system_instruction,\n",
        "        temperature=0.0,\n",
        "    )\n",
        ")\n",
        "\n",
        "res = response.text\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW-BNCAC2JzE"
      },
      "source": [
        "# Without Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr5zXEGIMwJu"
      },
      "source": [
        "Test the Gemini API to answer the same question without the addition of retrieved documents. Basically, the LLM will use its knowledge to answer the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZTmTtxcG9Rc",
        "outputId": "ce65bc7c-8784-409e-a185-2f9b14e835ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaMA 4 has not been released. The latest model in the LLaMA series is LLaMA 3.\n"
          ]
        }
      ],
      "source": [
        "# Test without retrieved documents\n",
        "QUESTION = \"How many parameters does LLaMA 4 model have?\"\n",
        "\n",
        "# System instructions\n",
        "system_instruction = \"You are an assistant and expert in answering questions.\"\n",
        "\n",
        "# Simple prompt without context\n",
        "prompt = \"Be concise and take your time to answer the following question. \\nQuestion: {}\\nAnswer:\"\n",
        "formatted_prompt = prompt.format(QUESTION)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=formatted_prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        system_instruction=system_instruction,\n",
        "        temperature=0.0,\n",
        "    )\n",
        ")\n",
        "\n",
        "res = response.text\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCEapX5C83m2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}