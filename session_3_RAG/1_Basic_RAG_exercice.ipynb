{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "## Overview\n",
        "This notebook presents a complete workflow for building a RAG system that enhances Large Language Models with external knowledge. You'll implement the core components that allow LLMs to access and use information beyond their training data!\n",
        "\n",
        "**What You'll Build:**\n",
        "- A text chunking function for document processing\n",
        "- An embedding generation system using OpenAI's API\n",
        "- A similarity-based retrieval mechanism\n",
        "- A prompt augmentation pipeline for context-aware responses\n"
      ],
      "metadata": {
        "id": "C1ScXJDkjaWF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tw3tvMs6R-Y"
      },
      "source": [
        "## Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HaB4G9zr0BYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52839c8-49e4-4736-f344-3069c6603ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==1.59.8 cohere==5.13.8 tiktoken==0.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYvUA6CF2Le6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" and \"GOOGLE_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] =  \"<YOUR_GOOGLE_API_KEY>\"\n",
        "\n",
        "\n",
        "\n",
        "# from google.colab import userdata\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] =  userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ViVXXIqXBai"
      },
      "outputs": [],
      "source": [
        "# False: Generate the embedding for the dataset. (Associated cost with using OpenAI endpoint)\n",
        "# True: Load the dataset that already has the embedding vectors.\n",
        "load_embedding = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JpI7GiZ--Gw"
      },
      "source": [
        "## Download Dataset (JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT68BDYt-GkG"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6NEJT9S2OoH"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\n",
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles-with_embeddings.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYDd03Qn_clh"
      },
      "source": [
        "## Read File and Chunk Text\n",
        "\n",
        "### Exercise 1: Implement the text chunking function\n",
        "\n",
        "Text chunking is crucial in RAG systems because:\n",
        "- Language models have token limits\n",
        "- Smaller chunks allow for more precise retrieval\n",
        "- Overlapping chunks can preserve context at boundaries\n",
        "\n",
        "Your task: Implement a function that splits text into chunks of a specified size.\n",
        "Consider:\n",
        "- How to handle the last chunk if it's smaller than chunk_size\n",
        "- Whether to add overlap between chunks (optional enhancement)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bfhs5NMYr4N"
      },
      "outputs": [],
      "source": [
        "# Split the input text into chunks of specified size.\n",
        "def split_into_chunks(text, chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Split the input text into chunks of specified size.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be split\n",
        "    - chunk_size (int): The maximum size of each chunk in characters\n",
        "\n",
        "    Returns:\n",
        "    - chunks (list): A list of text chunks\n",
        "\n",
        "    TODO: Implement this function\n",
        "    Hint: Use a loop to iterate through the text in steps of chunk_size\n",
        "    Think about: How would you extract a substring from position i to i+chunk_size?\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # TODO: Your implementation here\n",
        "    # Step 1: Create a loop that goes from 0 to len(text) in steps of chunk_size\n",
        "    # Step 2: In each iteration, extract a substring from current position to current position + chunk_size\n",
        "    # Step 3: Append each chunk to the chunks list\n",
        "    # Step 4: Return the chunks list\n",
        "\n",
        "    pass  # Remove this line when implementing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcQ7Ge_XCuXa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "chunks = []\n",
        "\n",
        "# Load the file as a CSV\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "            # Skip header row\n",
        "        chunks.extend(split_into_chunks(row[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyJ_C672cSYs"
      },
      "outputs": [],
      "source": [
        "print(\"number of articles:\", idx)\n",
        "print(\"number of chunks:\", len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKdFSOb0NXjx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the JSON list to a Pandas Dataframe\n",
        "df = pd.DataFrame(chunks, columns=[\"chunk\"])\n",
        "\n",
        "df.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pFDgNdW9rO"
      },
      "source": [
        "## Generate Embedding\n",
        "### Exercise 2: Implement the embedding generation function\n",
        "\n",
        "Embeddings are numerical representations of text that capture semantic meaning:\n",
        "* OpenAI's models convert text into high-dimensional vectors (1536 dimensions for text-embedding-3-small)\n",
        "* Similar texts have similar vector representations\n",
        "* The distance between vectors indicates semantic similarity\n",
        "\n",
        "Your task: Implement a function that converts text to embeddings using OpenAI's API.\n",
        "\n",
        "Important considerations:\n",
        "* Clean the text by removing newlines (they can affect embedding quality)\n",
        "* Use try-except to handle potential API errors gracefully\n",
        "* The OpenAI API might fail due to rate limits or network issues\n",
        "\n",
        "API details:\n",
        "* Use client.embeddings.create() method\n",
        "* Parameters: input=[your_text], model=\"text-embedding-3-small\"\n",
        "* Extract embedding from: response.data[0].embedding\n",
        "\n",
        "Reference: https://platform.openai.com/docs/guides/embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfS9w9eQAKyu"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "# Defining a function that converts a text to embedding vector using OpenAI's Ada model.\n",
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Convert a text to embedding vector using OpenAI's embedding model.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to convert to embedding\n",
        "\n",
        "    Returns:\n",
        "    - embedding (list): A list of floats representing the embedding vector\n",
        "\n",
        "    TODO: Implement this function\n",
        "    Important considerations:\n",
        "    1. Clean the text by removing newlines (they can affect embedding quality)\n",
        "    2. Use try-except to handle potential API errors gracefully\n",
        "    3. The OpenAI embeddings.create() method requires:\n",
        "       - input: a list containing your text\n",
        "       - model: use \"text-embedding-3-small\" for this exercise\n",
        "    4. The response contains the embedding in res.data[0].embedding\n",
        "\n",
        "    API Documentation: https://platform.openai.com/docs/guides/embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Your implementation here\n",
        "    # Step 1: Add try-except block for error handling\n",
        "    # Step 2: Inside try block, remove newlines from text using .replace()\n",
        "    # Step 3: Call client.embeddings.create() with appropriate parameters\n",
        "    # Step 4: Extract and return the embedding from the response\n",
        "    # Step 5: In except block, return None if an error occurs\n",
        "\n",
        "    pass  # Remove this line when implementing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC6aeFr3Rmi2"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Generate embedding\n",
        "if not load_embedding:\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings = []\n",
        "    for index, row in tqdm(df.iterrows()):\n",
        "        # df.at[index, 'embedding'] = get_embedding( row['chunk'] )\n",
        "        embeddings.append(get_embedding(row[\"chunk\"]))\n",
        "\n",
        "    embeddings_values = pd.Series(embeddings)\n",
        "    df.insert(loc=1, column=\"embedding\", value=embeddings_values)\n",
        "\n",
        "# Or, load the embedding from the file.\n",
        "else:\n",
        "    print(\"Loaded the embedding file.\")\n",
        "    # Load the file as a CSV\n",
        "    df = pd.read_csv(\"mini-llama-articles-with_embeddings.csv\")\n",
        "    # Convert embedding column to an array\n",
        "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(eval(x)), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyX9M_n9o2ve"
      },
      "outputs": [],
      "source": [
        "# df.to_csv('mini-llama-articles-with_embeddings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_qrXwImXrXJ"
      },
      "source": [
        "## User Question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGTa7cqCX97q"
      },
      "outputs": [],
      "source": [
        "# Define the user question, and convert it to embedding.\n",
        "QUESTION = \"How many parameters LLaMA2 model has?\"\n",
        "QUESTION_emb = get_embedding(QUESTION)\n",
        "\n",
        "len(QUESTION_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXNzNWrJYWhU"
      },
      "source": [
        "## Test Cosine Similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxaq-FgLIhIj"
      },
      "source": [
        "Calculating the similarity of embedding representations can help us to find pieces of text that are close to each other. In the following sample you see how the Cosine Similarity metric can identify which sentence could be a possible answer for the given user question. Obviously, the unrelated answer will score lower.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqDWcPd4b-ZI"
      },
      "outputs": [],
      "source": [
        "BAD_SOURCE_emb = get_embedding(\"The sky is blue.\")\n",
        "GOOD_SOURCE_emb = get_embedding(\"LLaMA2 model has a total of 2B parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI00eN86YZKB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# A sample that how a good piece of text can achieve high similarity score compared\n",
        "# to a completely unrelated text.\n",
        "print(\"> Bad Response Score:\", cosine_similarity([QUESTION_emb], [BAD_SOURCE_emb]))\n",
        "print(\"> Good Response Score:\", cosine_similarity([QUESTION_emb], [GOOD_SOURCE_emb]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdJlEtaaJC4I"
      },
      "source": [
        "## Calculate Cosine Similarities\n",
        "\n",
        "### Exercise 3: Implement the retrieval logic\n",
        "\n",
        "This is the core of the RAG system - finding the most relevant chunks to answer the user's question.\n",
        "\n",
        "Your task: Find the indices of the top N most similar chunks.\n",
        "\n",
        "Key concepts:\n",
        "* Cosine similarity returns a 2D array of similarity scores\n",
        "* Higher scores indicate more relevant chunks\n",
        "* You need to sort and select the best matches\n",
        "\n",
        "Implementation hints:\n",
        "* numpy's argsort() returns indices that would sort an array\n",
        "* To get descending order, you can use negative indexing: [::-1]\n",
        "* Select only the first N indices after sorting\n",
        "\n",
        "Example:\n",
        "* If scores = [0.3, 0.8, 0.5, 0.9], then np.argsort(scores) returns [0, 2, 1, 3]\n",
        "* To get descending order: np.argsort(scores)[::-1] returns [3, 1, 2, 0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNPN7OAXemmH"
      },
      "outputs": [],
      "source": [
        "# Calculate similarity between the question and each chunk\n",
        "cosine_similarities = cosine_similarity([QUESTION_emb], df[\"embedding\"].tolist())\n",
        "\n",
        "print(f\"Shape of similarities: {cosine_similarities.shape}\")\n",
        "\n",
        "number_of_chunks_to_retrieve = 3\n",
        "\n",
        "# TODO: Your implementation here\n",
        "# Step 1: Use np.argsort() on cosine_similarities[0] (the first and only row)\n",
        "# Step 2: Reverse the order to get descending sort (highest scores first)\n",
        "# Step 3: Select only the first 'number_of_chunks_to_retrieve' indices\n",
        "# Store the result in a variable called 'indices'\n",
        "\n",
        "indices = None  # Replace with your implementation\n",
        "\n",
        "# Display the retrieved chunks\n",
        "print(\"Retrieved chunks:\")\n",
        "for idx, item in enumerate(df.chunk[indices]):\n",
        "    print(f\"\\n> Chunk {idx+1}\")\n",
        "    print(item)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uvQACqAkHg4"
      },
      "source": [
        "## Augment the Prompt\n",
        "\n",
        "This is where RAG differs from standard LLM usage. Instead of relying only on the model's training data, we augment the prompt with retrieved context.\n",
        "\n",
        "Your task: Create an effective prompt that incorporates the retrieved chunks.\n",
        "\n",
        "Key components of a good RAG prompt:\n",
        "* Clear system instructions about using provided context\n",
        "* Explicit context boundaries (START/END tags)\n",
        "* The retrieved information placed before the question\n",
        "* Instructions to be accurate and base answers on the context\n",
        "\n",
        "Prompt engineering tips:\n",
        "* Be explicit about what the model should do with the context\n",
        "* Use clear delimiters to separate context from the question\n",
        "* Instruct the model to decline if information isn't in the context\n",
        "* Keep instructions concise but clear\n",
        "\n",
        "Template structure:\n",
        "```\n",
        "System: [Role and constraints]\n",
        "User: [Context] + [Question] + [Instructions]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw1lb0dJ6vP7"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "try:\n",
        "    # TODO: Define the system_prompt\n",
        "    # Should instruct the model to:\n",
        "    # - Answer questions based on provided chunks\n",
        "    # - Only answer AI-related questions\n",
        "    # - Decline non-AI questions politely\n",
        "\n",
        "    system_prompt = \"\"  # Your implementation here\n",
        "\n",
        "    # TODO: Create the prompt template\n",
        "    # Should include:\n",
        "    # - Instructions about using the context between <START_OF_CONTEXT> and <END_OF_CONTEXT>\n",
        "    # - Placeholders for context and question (use {} for .format())\n",
        "    # - Instructions to be accurate and concise\n",
        "    # Example structure:\n",
        "    # \"Read the following...<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\nQuestion: {}\\nAnswer:\"\n",
        "\n",
        "    prompt = \"\"  # Your implementation here\n",
        "\n",
        "    # TODO: Format the prompt with retrieved chunks and question\n",
        "    # Hint: Use \"\".join(df.chunk[indices]) to combine chunks\n",
        "    # formatted_prompt = prompt.format(???, ???)\n",
        "\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
        "\n",
        "    # Uncomment the following lines after implementing the prompt\n",
        "    # result = model.generate_content(formatted_prompt, request_options={\"timeout\": 1000})\n",
        "    # res = result.text\n",
        "    # print(res)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tBvJ8oMucha"
      },
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aamim8n6RZHO"
      },
      "source": [
        "## How Augmenting the Prompt can address knowledge cutoff limitations and hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAPOfX9fSKai"
      },
      "outputs": [],
      "source": [
        "# Consider this as a retrieved chunk\n",
        "# https://ai.meta.com/blog/meta-llama-3-1/\n",
        "Example_chunk = \"\"\"\n",
        "Introducing Llama 3.1 Llama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. With the release of the 405B model, we’re poised to supercharge innovation—with unprecedented opportunities for growth and exploration. We believe the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation—a capability that has never been achieved at this scale in open source.\n",
        "As part of this latest release, we’re introducing upgraded versions of the 8B and 70B models. These are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables our latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. We’ve also made changes to our license, allowing developers to use the outputs from Llama models—including the 405B—to improve other models. True to our commitment to open source, starting today, we’re making these models available to the community for download on llama.meta.com and Hugging Face and available for immediate development on our broad ecosystem of partner platforms. Model evaluations\n",
        "For this release, we evaluated performance on over 150 benchmark datasets that span a wide range of languages. In addition, we performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Our experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, our smaller models are competitive with closed and open models that have a similar number of parameters.\n",
        "Model Architecture As our largest model yet, training Llama 3.1 405B on over 15 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\n",
        "To address this, we made design choices that focus on keeping the model development process scalable and straightforward. We opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability.\n",
        "We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\n",
        "Compared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E28JMT8mSKUf"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"How many parameters LLaMA 3.1 model has?\"\n",
        "\n",
        "# Formulating the system prompt\n",
        "system_prompt = (\n",
        "        \"You are an assistant and expert in answering questions from a chunks of content. \"\n",
        "        \"Only answer AI-related question, else say that you cannot answer this question.\"\n",
        "    )\n",
        "# Combining the system prompt with the user's question\n",
        "prompt = (\n",
        "        \"Read the following informations that might contain the context you require to answer the question. You can use the informations starting from the <START_OF_CONTEXT> tag and end with the <END_OF_CONTEXT> tag. Here is the content:\\n\\n<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\n\"\n",
        "        \"Please provide an informative and accurate answer to the following question based on the avaiable context. Be concise and take your time. \\nQuestion: {}\\nAnswer:\"\n",
        "    )\n",
        "prompt = prompt.format(Example_chunk, QUESTION)\n",
        "\n",
        "model = genai.GenerativeModel(model_name= \"gemini-1.5-flash\", system_instruction=system_prompt)\n",
        "\n",
        "#Gemini API call\n",
        "result = model.generate_content(prompt,request_options={\"timeout\": 1000},)\n",
        "res = result.text\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW-BNCAC2JzE"
      },
      "source": [
        "# Without Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr5zXEGIMwJu"
      },
      "source": [
        "Test the Gemini API to answer the same question without the addition of retrieved documents. Basically, the LLM will use its knowledge to answer the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuyXjzZyuecE"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"How many parameters LLaMA 3 model has?\"\n",
        "\n",
        "# Formulating the system prompt\n",
        "system_prompt = \"You are an assistant and expert in answering questions.\"\n",
        "\n",
        "# Combining the system prompt with the user's question\n",
        "prompt = \"Be concise and take your time to answer the following question. \\nQuestion: {}\\nAnswer:\"\n",
        "prompt = prompt.format(QUESTION)\n",
        "\n",
        "model = genai.GenerativeModel(model_name= \"gemini-1.5-flash\", system_instruction=system_prompt)\n",
        "\n",
        "#Gemini API call\n",
        "result = model.generate_content(prompt,request_options={\"timeout\": 1000},)\n",
        "res = result.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAy34tPTzGbh"
      },
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection & Next Steps\n",
        "\n",
        "### Key Questions to Consider\n",
        "\n",
        "1. **Chunking Strategy**: How might different chunk sizes affect retrieval quality? What about overlapping chunks?\n",
        "\n",
        "2. **Embedding Quality**: Why do we remove newlines before generating embeddings? How might preprocessing affect results?\n",
        "\n",
        "3. **Retrieval Precision**: We used top-3 chunks. How would you determine the optimal number of chunks to retrieve?\n",
        "\n",
        "4. **Prompt Engineering**: How does the structure of your augmented prompt affect the model's response quality?\n",
        "\n",
        "5. **Limitations**: What are potential drawbacks of this basic RAG approach? How might you improve it?\n",
        "\n",
        "### Bonus Challenges\n",
        "\n",
        "If you finish early, try these enhancements:\n",
        "\n",
        "1. **Overlap Implementation**: Modify the chunking function to include overlap between chunks\n",
        "   ```python\n",
        "   def split_into_chunks_with_overlap(text, chunk_size=1024, overlap=128):\n",
        "       # Implement sliding window approach\n",
        "   ```\n",
        "\n",
        "2. **Similarity Threshold**: Instead of top-N, retrieve chunks above a similarity threshold\n",
        "   ```python\n",
        "   threshold = 0.7\n",
        "   relevant_indices = np.where(cosine_similarities[0] > threshold)[0]\n",
        "   ```\n",
        "\n",
        "3. **Reranking**: Implement a secondary ranking step after initial retrieval using a different similarity metric\n",
        "\n",
        "4. **Evaluation**: Create a function to evaluate answer quality with and without RAG using metrics like:\n",
        "   * Answer relevance\n",
        "   * Information completeness\n",
        "   * Factual accuracy\n",
        "\n",
        "### Future Enhancements\n",
        "\n",
        "This basic RAG implementation can be enhanced with:\n",
        "* Vector databases (Pinecone, Weaviate, ChromaDB)\n",
        "* More sophisticated chunking strategies\n",
        "* Hybrid search (combining dense and sparse retrieval)\n",
        "* Query expansion and reformulation\n",
        "* Multi-step reasoning with chain-of-thought\n"
      ],
      "metadata": {
        "id": "eJQtjxdEoy15"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4Tw3tvMs6R-Y",
        "5JpI7GiZ--Gw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}